tapply(movies$Musical, clusterGroups, mean)
tapply(movies$Mystery, clusterGroups, mean)
tapply(movies$SciFi, clusterGroups, mean)
tapply(movies$Thriller, clusterGroups, mean)
tapply(movies$War, clusterGroups, mean)
tapply(movies$western, clusterGroups, mean)
tapply(movies$Western, clusterGroups, mean)
subset(movies, Title=="Men in Black (1997)")
clusterGroups[257]
cluster2 = subset(movies, clusterGroups==2)
# Look at the first 10 titles in this cluster:
cluster2$Title[1:10]
cluster2$Title[1:]
cluster2$Title[1:10]
cluster2$Title
# Assign points to clusters
twoGroups = cutree(clusterMovies, k = 2)
twoGroups
tapply(movies$Action, twoGroups, mean)
tapply(movies$Romance, twoGroups, mean)
tapply(movies$Adventure, twoGroups, mean)
tapply(movies$Animation, twoGroups, mean)
tapply(movies$Childrens, twoGroups, mean)
tapply(movies$Comedy, twoGroups, mean)
tapply(movies$Crime, twoGroups, mean)
tapply(movies$Documentary, twoGroups, mean)
tapply(movies$Drama, twoGroups, mean)
tapply(movies$Fantasy, twoGroups, mean)
tapply(movies$FilmNoir, twoGroups, mean)
tapply(movies$Horror, twoGroups, mean)
tapply(movies$Musical, twoGroups, mean)
tapply(movies$Mystery, twoGroups, mean)
tapply(movies$SciFi, twoGroups, mean)
tapply(movies$Thriller, twoGroups, mean)
tapply(movies$War, twoGroups, mean)
tapply(movies$Western, twoGroups, mean)
library(rUnemploymentData)
data(df_county_unemployment)
ylab="Percent Unemployment")
boxplot(df_county_unemployment[, c(-1, -2, -3)],main="USA County Unemployment Data",xlab="Year",ylab="Percent Unemployment")
county_unemployment_choropleth(year=2013)
animated_county_unemployment_choropleth()
county_unemployment_choropleth(year=2009)
[1,-3]*[4,5]
c[1,-3]*c[4,5]
(1,-3)*(4,5)
as.matrix(1,-3)*as.matrix(4,5)
a <-as.martix(1,-3)
a <-as.matrix(1,-3)
a
a<-matrix(data = c(1,-3), nrow = 1, ncol = 2, byrow = TRUE, dimnames = NULL)
a
b<-matrix(data = c(4,5), nrow = 1, ncol = 2, byrow = TRUE, dimnames = NULL)
a*b
a<-matrix(data = c(3,4,5), nrow = 1, ncol = 3, byrow = TRUE, dimnames = NULL)
b<-matrix(data = c(1,2,3), nrow = 1, ncol = 3, byrow = TRUE, dimnames = NULL)
a*b
sum(a)
sum(a*b)
dotproduct <- function(dataf, v2) {
>        apply(t(t(as.matrix(a)) * v2),1,sum) #contorted!
>    }
>
>    df = data.frame(a=c(1,2,3),b=c(4,5,6))
>    vec = c(4,5)
>    dotproduct(df, vec)
dotproduct <- function(dataf, v2) {
apply(t(t(as.matrix(a)) * v2),1,sum) #contorted!
}
df = data.frame(a=c(1,2,3),b=c(4,5,6))
vec = c(4,5)
View(df)
df = data.frame(a=3),b=5,c=5)
df = data.frame(a=3,b=5,c=5)
vec = c(1,2,3)
dotproduct(df, vec)
df = data.frame(a=1,b=-3)
vec = c(4,5)
dotproduct(df, vec)
sum(df*vec)
b<-matrix(data = c(1,2,3), nrow = 1, ncol = 3, byrow = TRUE, dimnames = NULL)
x<-matrix(data = c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = TRUE, dimnames = NULL)
x
y<-matrix(data = c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = FALSE, dimnames = NULL)
y
y<-matrix(data = c(1,2,3,4,5,6), nrow = 3, ncol = 2, byrow = FALSE, dimnames = NULL)
y
y<-matrix(data = c(1,2,3,4,5,6), nrow = 3, ncol = 2, byrow = TRUE, dimnames = NULL)
y
x*y
matrix(x)*matrix(y)
y<-matrix(data = c(1,2,3), nrow = 1, ncol = 1, byrow = TRUE, dimnames = NULL)
y
y<-matrix(data = c(1,2,3), nrow = 1, ncol = 1, byrow = FALSE, dimnames = NULL)
y
y<-matrix(data = c(1,2,3), nrow = 1, ncol = 1, byrow = TRUE, dimnames = NULL)
y
y<-matrix(data = c(1,2,3), nrow = 3, ncol = 1, byrow = FALSE, dimnames = NULL)
y
x<-matrix(data = c(1,2,3), nrow = 1, ncol = 3, byrow = FALSE, dimnames = NULL)
x
matrix(x)*matrix(y)
x*y
matrix(x)*matrix(y)
library(rUnemploymentData)
data(df_county_unemployment)
View(df_county_unemployment)
county_unemployment_choropleth(year=2008)
data
df_county_unemployment["region"==2105]
df_county_unemployment[region==2105]
df<- df_county_unemployment
df[,0]
df[[0]]
df[[,0]]
df[,0]
View(df)
df[[,1]]
df[,1]
df[[,1]==5106]
df[[,1 ==5106]
]
df[[,1]==5106
df[[,1]]==5106
df[[1]==5106
df[1]==5106
county_unemployment_choropleth(year=2008)
boxplot(df_county_unemployment[, c(-1, -2, -3)],main="USA County Unemployment Data",xlab="Year",ylab="Percent Unemployment")
county_unemployment_choropleth(region=2105)
county_unemployment_choropleth(year=2008)
#animated_county_unemployment_choropleth()
notin<-df[1]==c(2105,2198,15005,2195,2275)
notin
df2<-df[notin]
df2<-df[notin,]
View(df2)
source('functions.R')
# Load the data
df<-data(df_county_unemployment)
df<-cleanit(df)
summary(df)
df<-cleanit(df_county_unemployment)
summary(df)
# count blanks remove blanks
barplot(colSums(!is.na(df)))
#df <- na.omit(df)
colSums(!is.na(df))
colSums(!is.na(df))
df <- na.omit(df)
colSums(!is.na(df))
county_unemployment_choropleth(year=2008)
source('functions.R')
# Load the data
data(df_county_unemployment)
df_county_unemployment<-cleanit(df_county_unemployment)
summary(df)
summary(df_county_unemployment)
#df <- na.omit(df)
colSums(!is.na(df))
barplot(colSums(!is.na(df_county_unemployment)))
#df <- na.omit(df)
colSums(!is.na(df_county_unemployment))
boxplot(df_county_unemployment[, c(-1, -2, -3)],main="USA County Unemployment Data",xlab="Year",ylab="Percent Unemployment")
county_unemployment_choropleth(year=2008)
?rUnemploymentData
load("D:/Data/censusdata/admin1.regions.rdata")
View(admin1.regions)
source('~/.active-rstudio-document', echo=TRUE)
build_county_df
build_county_df()
source('~/.active-rstudio-document', echo=TRUE)
build_county_df()
?trim
?str
trim?
?trim
?rtrim
install.packages("stringr")
install.packages("stringr")
source('~/.active-rstudio-document', echo=TRUE)
build_county_df()
library(strngr)
library(stringr)
build_county_df()
warnings()
install.packages('pollstR', dependencies = TRUE)
library("pollstR")
library(stringi)
charts <- pollstr_charts()
str(charts)
charts <- pollstr_charts()
str(charts)
us_charts <- pollstr_charts(state = "US")
obama_favorable <- pollstr_chart('obama-favorable-rating')
print(obama_favorable)
(ggplot(obama_favorable[["estimates_by_date"]], aes(x = date, y = value, color = choice))
+ geom_line())
library(ggplot)
install.packages("ggplot")
ps aux
exit
exit
quit()
ls -la
ps aux
exit
quit()
# KAGGLE COMPETITION - GETTING STARTED
# This script file is intended to help you get started on the Kaggle platform, and to show you how to make a submission to the competition.
# Load functions
source('C:/Users/bryan_000/Documents/GitHub/MyWork/functions.R')
# Set Directory
setwd("C:/Users/bryan_000/Documents/GitHub/Kcomp")
# Let's start by reading the data into R
# Make sure you have downloaded these files from the Kaggle website, and have navigated to the directory where you saved the files on your computer
# We are adding in the argument stringsAsFactors=FALSE, since we have some text fields
dfTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
dfTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
# count blanks remove blanks
barplot(colSums(!is.na(dfTrain)))
colSums(!is.na(dfTrain))
#df <- na.omit(df)
#SPlit training on
dfTrain1 <-subset(dfTrain,biddable==1)
dfTrain2 <-subset(dfTrain,biddable==0)
# Subset test on biddable
dfTest1 <-subset(dfTest,biddable==1)
dfTest2 <-subset(dfTest,biddable==0)
# Baseline on Training data
# Determine the Majority
bl <-table(dfTrain$sold)
majority<-ifelse(bl[1]>bl[2],0,1)
# Fill in a prediction for the majority
predictTrainBase <-rep(majority,nrow(dfTrain))
#Compare
cm <- table(dfTrain$sold,predictTrainBase, exclude=NULL)
addmargins(cm)
getstats(cm)
# Dep and Independent Vars define columns we will be working with
depvar <- 'sold'
indepvars <-c('biddable','startprice', 'condition', 'cellular', 'storage','productline')
exclude <- c('UniqueID') # numerical variables to exclude from using all
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
f2 <- paste(f1,paste(exclude,collapse=' - '),sep=' - ')
# We will just create a simple logistic regression model, to predict Sold using Price:
model1 <- glm(f1, family=binomial, data=dfTrain1)
model2 <- glm(f1, family=binomial, data=dfTrain2)
# Predict on Training
thres = .5
predictTrain <- predict(SimpleMod, type="response")
cm <- table(dfTrain$sold,predictTrain>thres)
addmargins(cm)
getstats(cm)
#Predict separate for biddable
thres = .5
predictTrain1 <- predict(model1, type="response")
cm1 <- table(dfTrain1$sold,predictTrain1>thres)
addmargins(cm1)
getstats(cm1)
thres = .5
predictTrain2 <- predict(model2, type="response")
cm2 <- table(dfTrain2$sold,predictTrain2>thres)
addmargins(cm2)
getstats(cm2)
# And then make predictions on the test set:
model1pred <- predict(model1, newdata=dfTest1, type="response")
model1pred <- predict(model2, newdata=dfTest2, type="response")
model1pred <- predict(model1, newdata=dfTest1, type="response")
model2pred <- predict(model2, newdata=dfTest2, type="response")
model1pred
model2pred
model1pred
MySubmission = data.frame(UniqueID = dfTest1$UniqueID, Probability1 = model1pred)
write.csv(MySubmission, "SubmissionSimpleLog1.csv", row.names=FALSE)
MySubmission = data.frame(UniqueID = dfTest2$UniqueID, Probability1 = model2pred)
write.csv(MySubmission, "SubmissionSimpleLog1.csv", row.names=FALSE)
model1pred <- predict(model1, newdata=dfTest1, type="response")
model2pred <- predict(model2, newdata=dfTest2, type="response")
# We can't compute the accuracy or AUC on the test set ourselves, since we don't have the dependent variable on the test set (you can compute it on the training set though!).
# However, you can submit the file on Kaggle to see how well the model performs. You can make up to 5 submissions per day, so don't hesitate to just upload a solution to see how you did.
# Let's prepare a submission file for Kaggle (for more about this, see the "Evaluation" page on the competition site):
MySubmission = data.frame(UniqueID = dfTest1$UniqueID, Probability1 = model1pred)
write.csv(MySubmission, "SubmissionSimpleLog1.csv", row.names=FALSE)
MySubmission = data.frame(UniqueID = dfTest2$UniqueID, Probability1 = model2pred)
write.csv(MySubmission, "SubmissionSimpleLog2.csv", row.names=FALSE)
depvar <- 'sold'
indepvars <-c('biddable','startprice', 'condition', 'cellular', 'storage','productline','carrier')
exclude <- c('UniqueID') # numerical variables to exclude from using all
f1 <- paste(depvar,paste(indepvars,collapse=' + '),sep=' ~ ')
f2 <- paste(f1,paste(exclude,collapse=' - '),sep=' - ')
# We will just create a simple logistic regression model, to predict Sold using Price:
SimpleMod <- step(glm(f1, data=dfTrain, family=binomial))
model1 <- glm(f1, family=binomial, data=dfTrain1)
model2 <- glm(f1, family=binomial, data=dfTrain2)
# Predict on Training
thres = .5
predictTrain <- predict(SimpleMod, type="response")
cm <- table(dfTrain$sold,predictTrain>thres)
addmargins(cm)
getstats(cm)
#Predict separate for biddable
thres = .5
predictTrain1 <- predict(model1, type="response")
cm1 <- table(dfTrain1$sold,predictTrain1>thres)
addmargins(cm1)
getstats(cm1)
thres = .5
predictTrain2 <- predict(model2, type="response")
cm2 <- table(dfTrain2$sold,predictTrain2>thres)
addmargins(cm2)
getstats(cm2)
# And then make predictions on the test set:
PredTest = predict(SimpleMod, newdata=dfTest, type="response")
model1pred <- predict(model1, newdata=dfTest1, type="response")
model2pred <- predict(model2, newdata=dfTest2, type="response")
# We can't compute the accuracy or AUC on the test set ourselves, since we don't have the dependent variable on the test set (you can compute it on the training set though!).
# However, you can submit the file on Kaggle to see how well the model performs. You can make up to 5 submissions per day, so don't hesitate to just upload a solution to see how you did.
# Let's prepare a submission file for Kaggle (for more about this, see the "Evaluation" page on the competition site):
MySubmission = data.frame(UniqueID = dfTest1$UniqueID, Probability1 = model1pred)
write.csv(MySubmission, "SubmissionSimpleLog1.csv", row.names=FALSE)
MySubmission = data.frame(UniqueID = dfTest2$UniqueID, Probability1 = model2pred)
write.csv(MySubmission, "SubmissionSimpleLog2.csv", row.names=FALSE)
# You should upload the submission "SubmissionSimpleLog.csv" on the Kaggle website to use this as a submission to the competition
# This model was just designed to help you get started - to do well in the competition, you will need to build better models!
library(rpart)
library(rpart.plot)
treeFit <-rpart(isB ~ . - letter, data=dfTrain, method="class")
#treeFit <-rpart(isB ~ . - letter, data=dfTrain, method="class")
treeFit <-rpart(f1, data=dfTrain, method="class")
cm <-table(dfTrain$sold, predictTrain)
addmargins(cm)
getstats(cm)
predictTrain = predict(treeFit, type = "class")
cm <-table(dfTrain$sold, predictTrain)
addmargins(cm)
getstats(cm)
# And then make predictions on the test set:
PredTest = predict(treeFit, newdata=dfTest, type="class")
treeFit <-rpart(f1, data=dfTrain1, method="class")
predictTrain = predict(treeFit, type = "class")
cm <-table(dfTrain$sold, predictTrain)
addmargins(cm)
getstats(cm)
treeFit <-rpart(f1, data=dfTrain1, method="class")
predictTrain = predict(treeFit, type = "class")
cm <-table(dfTrain$sold, predictTrain)
cm <-table(dfTrain1$sold, predictTrain)
addmargins(cm)
getstats(cm)
treeFit <-rpart(f1, data=dfTrain2, method="class")
predictTrain = predict(treeFit, type = "class")
cm <-table(dfTrain2$sold, predictTrain)
addmargins(cm)
getstats(cm)
# And then make predictions on the test set:
PredTest = predict(treeFit, newdata=dfTest, type="class")
library(rpart)
library(rpart.plot)
#treeFit <-rpart(isB ~ . - letter, data=dfTrain, method="class")
treeFit1 <-rpart(f1, data=dfTrain1, method="class")
treeFit2 <-rpart(f1, data=dfTrain2, method="class")
#treeFit <-rpart(isB ~ . - letter, data=dfTrain, method="class")
treeFit1 <-rpart(f1, data=dfTrain1, method="class")
treeFit2 <-rpart(f1, data=dfTrain2, method="class")
#Predict on Training
predictTrain1 = predict(treeFit1, type = "class")
cm <-table(dfTrain1$sold, predictTrain)
addmargins(cm)
#treeFit <-rpart(isB ~ . - letter, data=dfTrain, method="class")
treeFit1 <-rpart(f1, data=dfTrain1, method="class")
treeFit2 <-rpart(f1, data=dfTrain2, method="class")
#Predict on Training
predictTrain1 = predict(treeFit1, type = "class")
cm <-table(dfTrain1$sold, predictTrain1)
addmargins(cm)
getstats(cm)
#redict on Traing
predictTrain2 = predict(treeFit2, type = "class")
cm <-table(dfTrain2$sold, predictTrain2)
addmargins(cm)
getstats(cm)
# And then make predictions on the test set:
PredTest1 = predict(treeFit1, newdata=dfTest1, type="class")
PredTest2 = predict(treeFit2, newdata=dfTest2, type="class")
# Let's prepare a submission file for Kaggle (for more about this, see the "Evaluation" page on the competition site):
MySubmission = data.frame(UniqueID = dfTest1$UniqueID, Probability1 = PredTest1)
write.csv(MySubmission, "SubmissionSimpleRF1.csv", row.names=FALSE)
MySubmission = data.frame(UniqueID = dfTest2$UniqueID, Probability1 = PredTest2)
write.csv(MySubmission, "SubmissionSimpleRF2.csv", row.names=FALSE)
PredTest1
PredTest1 = predict(treeFit1, newdata=dfTest1, type="prob")
PredTest2 = predict(treeFit2, newdata=dfTest2, type="prob")
# Let's prepare a submission file for Kaggle (for more about this, see the "Evaluation" page on the competition site):
MySubmission = data.frame(UniqueID = dfTest1$UniqueID, Probability1 = PredTest1)
write.csv(MySubmission, "SubmissionSimpleRF1.csv", row.names=FALSE)
MySubmission = data.frame(UniqueID = dfTest2$UniqueID, Probability1 = PredTest2)
write.csv(MySubmission, "SubmissionSimpleRF2.csv", row.names=FALSE)
# KAGGLE COMPETITION - DEALING WITH THE TEXT DATA
# This script file is intended to help you deal with the text data provided in the competition data files
# If you haven't already, start by reading the data into R
# Make sure you have downloaded these files from the Kaggle website, and have navigated to the directory where you saved the files on your computer
# Load functions
source('C:/Users/bryan_000/Documents/GitHub/MyWork/functions.R')
# Set Directory
setwd("C:/Users/bryan_000/Documents/GitHub/Kcomp")
# We are adding in the argument stringsAsFactors=FALSE, since we have some text fields
dfTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
dfTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
# Now, let's load the "tm" package.
library(tm)
# Then create a corpus from the description variable. You can use other variables in the dataset for text analytics, but we will just show you how to use this particular variable.
# Note that we are creating a corpus out of the training and testing data.
CorpusDescription = Corpus(VectorSource(c(dfTrain$description, dfTest$description)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusDescription = tm_map(CorpusDescription, content_transformer(tolower), lazy=TRUE)
# Remember this extra line is needed after running the tolower step:
CorpusDescription = tm_map(CorpusDescription, PlainTextDocument, lazy=TRUE)
CorpusDescription = tm_map(CorpusDescription, removePunctuation, lazy=TRUE)
CorpusDescription = tm_map(CorpusDescription, removeWords, stopwords("english"), lazy=TRUE)
CorpusDescription = tm_map(CorpusDescription, stemDocument, lazy=TRUE)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusDescription)
sparse = removeSparseTerms(dtm, 0.99)
DescriptionWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(DescriptionWords) = make.names(colnames(DescriptionWords))
# Now we need to split the observations back into the training set and testing set.
# To do this, we can use the head and tail functions in R.
# The head function takes the first "n" rows of DescriptionWords (the first argument to the head function), where "n" is specified by the second argument to the head function.
# So here we are taking the first nrow(eBayTrain) observations from DescriptionWords, and putting them in a new data frame called "DescriptionWordsTrain"
DescriptionWordsTrain = head(DescriptionWords, nrow(dfTrain))
# The tail function takes the last "n" rows of DescriptionWords (the first argument to the tail function), where "n" is specified by the second argument to the tail function.
# So here we are taking the last nrow(eBayTest) observations from DescriptionWords, and putting them in a new data frame called "DescriptionWordsTest"
DescriptionWordsTest = tail(DescriptionWords, nrow(dfTest))
# Note that this split of DescriptionWords works to properly put the observations back into the training and testing sets, because of how we combined them together when we first made our corpus.
# Before building models, we want to add back the original variables from our datasets. We'll add back the dependent variable to the training set, and the WordCount variable to both datasets. You might want to add back more variables to use in your model - we'll leave this up to you!
DescriptionWordsTrain$sold = dfTrain$sold
DescriptionWordsTrain$WordCount = dfTrain$WordCount
DescriptionWordsTest$WordCount =dfTest$WordCount
# Added Back as Well
DescriptionWordsTrain$biddable <-dfTrain$biddable
DescriptionWordsTest$biddable <-dfTest$biddable
DescriptionWordsTrain$startprice <-dfTrain$startprice
DescriptionWordsTest$startprice <- dfTest$startprice
DescriptionWordsTrain$cellular <- dfTrain$cellular
DescriptionWordsTest$cellular <- dfTest$cellular
DescriptionWordsTrain$storage <- dfTrain$storage
DescriptionWordsTest$storage <- dfTest$storage
DescriptionWordsTest$UniqueID <- dfTest$UniqueID
DescriptionWordsTrain$carrier <- dfTrain$carrier
DescriptionWordsTest$carrier <- dfTest$carrier
DescriptionWordsTrain$condition <- dfTrain$condition
DescriptionWordsTest$condition <- dfTest$condition
DescriptionWordsTrain$productline <- dfTrain$productline
DescriptionWordsTest$productline <- dfTest$productline
# Remember that you can always look at the structure of these data frames to understand what we have created
# Baseline on Training data
# Determine the Majority
bl <-table(DescriptionWordsTrain$sold)
majority<-ifelse(bl[1]>bl[2],0,1)
# Fill in a prediction for the majority
predictTrainBase <-rep(majority,nrow(DescriptionWordsTrain))
#Compare
cm <- table(DescriptionWordsTrain$sold,predictTrainBase, exclude=NULL)
addmargins(cm)
getstats(cm)
#SPlit training on biddable
DescriptionWordsTrain1 <-subset(DescriptionWordsTrain,biddable==1)
DescriptionWordsTrain2 <-subset(DescriptionWordsTrain,biddable==0)
# Subset test on biddable
DescriptionWordsTest1 <-subset(DescriptionWordsTest,biddable==1)
DescriptionWordsTest2 <-subset(DescriptionWordsTest,biddable==0)
bl <-table(DescriptionWordsTrain1$sold)
majority<-ifelse(bl[1]>bl[2],0,1)
# Fill in a prediction for the majority
predictTrainBase <-rep(majority,nrow(DescriptionWordsTrain1))
#Compare
cm <- table(DescriptionWordsTrain$sold,predictTrainBase, exclude=NULL)
cm <- table(DescriptionWordsTrain1$sold,predictTrainBase, exclude=NULL)
addmargins(cm)
getstats(cm)
DescriptionWordsLog1 = glm(sold ~ ., data=DescriptionWordsTrain1, family=binomial)
DescriptionWordsLog2 = glm(sold ~ ., data=DescriptionWordsTrain2, family=binomial)
summary(DescriptionWordsLog1)
thres = .5
predictTrain1 <- predict(DescriptionWordsLog1, type="response")
cm <- table(DescriptionWordsTrain1$sold,predictTrain1>thres)
addmargins(cm)
getstats(cm)
predictTrain2 <- predict(DescriptionWordsLog2, type="response")
cm <- table(DescriptionWordsTrain2$sold,predictTrain2>thres)
addmargins(cm)
getstats(cm)
thres = .5
predictTrain1 <- predict(DescriptionWordsLog1, type="response")
cm <- table(DescriptionWordsTrain1$sold,predictTrain1>thres)
addmargins(cm)
getstats(cm)
thres = .5
predictTrain2 <- predict(DescriptionWordsLog2, type="response")
cm <- table(DescriptionWordsTrain2$sold,predictTrain2>thres)
addmargins(cm)
getstats(cm)
# And make predictions on our test set:
PredTest1 = predict(DescriptionWordsLog1, newdata=DescriptionWordsTest1, type="response")
PredTest2 = predict(DescriptionWordsLog2, newdata=DescriptionWordsTest2, type="response")
# Now we can prepare our submission file for Kaggle:
MySubmission = data.frame(UniqueID = DescriptionWordsTest1$UniqueID, Probability1 = PredTest1)
write.csv(MySubmission, "SubmissionDescriptionLogLetter1.csv", row.names=FALSE)
MySubmission = data.frame(UniqueID = DescriptionWordsTest2$UniqueID, Probability1 = PredTest2)
write.csv(MySubmission, "SubmissionDescriptionLogLetter2.csv", row.names=FALSE)
